# Main docker-compose.yml file for the AI development environment
# This file orchestrates several services that work together to provide
# a comprehensive AI development platform

# Define the main network our services will use to communicate
networks:
  lai:
    name: lai_local_ai_server
    driver: bridge

# Base configuration for n8n services that will be reused
x-n8n: &service-n8n
  image: n8nio/n8n:latest
  networks: ['lai']
  environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=postgres
    - DB_POSTGRESDB_USER=${POSTGRES_USER}
    - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
    - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY
    - N8N_USER_MANAGEMENT_JWT_SECRET
    - N8N_USER_MANAGEMENT_DISABLED=true
    - NODE_ENV=production
  links:
    - postgres

# Base configuration for Ollama services
x-ollama: &service-ollama
  image: ollama/ollama:latest
  container_name: lai-ollama
  networks: ['lai']
  restart: unless-stopped
  ports:
    - 11434:11434
  volumes:
    - ./ollama/models:/root/.ollama:rw

# Configuration for initializing Ollama with required models
x-init-ollama: &init-ollama
  image: ollama/ollama:latest
  networks: ['lai']
  container_name: lai-ollama-pull-llama
  volumes:
    - ./ollama/models:/root/.ollama:rw
  entrypoint: /bin/sh
  command:
    - "-c"
    - "sleep 3; OLLAMA_HOST=lai-ollama:11434 ollama pull llama3.1; OLLAMA_HOST=lai-ollama:11434 ollama pull nomic-embed-text"

# Main services section defining all our containers
services:
  # MongoDB with Vector Search capabilities
  mongodb:
    image: mongo:7.0
    container_name: lai-mongodb
    networks: ['lai']
    restart: unless-stopped
    ports:
      - 27017:27017
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGODB_USER:-root}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD:-password}
      - MONGO_INITDB_DATABASE=${MONGODB_DATABASE:-langchain_db}
    volumes:
      - ./mongodb/data:/data/db:rw
      - ./mongodb/init:/docker-entrypoint-initdb.d:ro
    command: mongod --bind_ip_all
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Flowise - Low code AI flow builder
  flowise:
    image: flowiseai/flowise
    networks: ['lai']
    restart: unless-stopped
    container_name: lai-flowise
    environment:
        - PORT=3001
        - FLOWISE_DATA_PATH=/root/.flowise
        - FLOWISE_STORAGE_PATH=/root/.flowise-storage
        - MONGODB_URI=mongodb://${MONGODB_USER:-root}:${MONGODB_PASSWORD:-password}@lai-mongodb:27017/${MONGODB_DATABASE:-langchain_db}?authSource=admin
    ports:
        - 3002:3001  # Changed to 3002 to avoid conflict with Perplexica
    extra_hosts:
      - "host.docker.internal:host-gateway"        
    volumes:
        - ./flowise/data:/root/.flowise:rw
        - ./flowise/storage:/root/.flowise-storage:rw
    entrypoint: /bin/sh -c "sleep 3; flowise start"
    depends_on:
      mongodb:
        condition: service_healthy

  # Open WebUI - Interface for interacting with AI models
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    networks: ['lai']
    restart: unless-stopped
    container_name: lai-open-webui
    ports:
      - "3003:8080"  # Changed to 3003 to avoid conflict with Perplexica
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./open-webui/data:/app/backend/data:rw

  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: lai-postgres
    networks: ['lai']
    restart: unless-stopped
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
    volumes:
      - ./postgres/data/pgdata:/var/lib/postgresql/data:rw
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-root} -d ${POSTGRES_DB:-n8n}']
      interval: 5s
      timeout: 5s
      retries: 10

  # n8n Workflow Importer
  n8n-import:
    <<: *service-n8n
    container_name: lai-n8n-import
    entrypoint: /bin/sh
    command:
      - "-c"
      - "sleep 10 && n8n import:credentials --separate --input=/backup/credentials && n8n import:workflow --separate --input=/backup/workflows"
    volumes:
      - ./n8n/backup:/backup:ro
    depends_on:
      postgres:
        condition: service_healthy

  # Main n8n Service
  n8n:
    <<: *service-n8n
    container_name: lai-n8n
    restart: unless-stopped
    ports:
      - 5678:5678
    volumes:
      - ./n8n/data:/home/node/.n8n:rw
      - ./n8n/backup:/backup:ro
      - ./shared:/data/shared:rw
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_USER=${POSTGRES_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_PERSONALIZATION_ENABLED=false
      - N8N_ENCRYPTION_KEY
      - N8N_USER_MANAGEMENT_JWT_SECRET
      - N8N_USER_MANAGEMENT_DISABLED=true
      - NODE_ENV=production
      - MONGODB_URI=mongodb://${MONGODB_USER:-root}:${MONGODB_PASSWORD:-password}@lai-mongodb:27017/${MONGODB_DATABASE:-langchain_db}?authSource=admin
    networks:
      - lai
    depends_on:
      postgres:
        condition: service_healthy
      n8n-import:
        condition: service_completed_successfully
      mongodb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:5678"]
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant
    container_name: lai-qdrant
    networks: ['lai']
    restart: unless-stopped
    ports:
      - 6333:6333
    volumes:
      - ./qdrant/storage:/qdrant/storage:rw

  # Ollama CPU Version
  ollama:
    <<: *service-ollama
    profiles: ["cpu"]

  # Ollama GPU Version
  ollama-gpu:
    <<: *service-ollama
    container_name: lai-ollama-gpu
    profiles: ["gpu-nvidia"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Initialize Ollama Models (CPU)
  ollama-pull-llama:
    <<: *init-ollama
    profiles: ["cpu"]
    depends_on:
      - ollama

  # Initialize Ollama Models (GPU)
  ollama-pull-llama-gpu:
    <<: *init-ollama
    container_name: lai-ollama-pull-llama-gpu
    profiles: ["gpu-nvidia"]
    depends_on:
      - ollama-gpu

  # SearxNG Search Engine
  searxng:
    image: searxng/searxng:latest
    container_name: lai-searxng
    networks: ['lai']
    volumes:
      - ./perplexica/searxng:/etc/searxng:rw
    ports:
      - 4000:8080
    restart: unless-stopped

  # Nginx Proxy Manager
  nginx-proxy-manager:
    image: 'jc21/nginx-proxy-manager:latest'
    container_name: lai-nginx
    networks: ['lai']
    restart: unless-stopped
    ports:
      - '80:80'
      - '81:81'
      - '443:443'
    volumes:
      - ./nginx/data:/data:rw
      - ./nginx/letsencrypt:/etc/letsencrypt:rw

  # Node-RED
  node-red:
    image: nodered/node-red:latest
    container_name: lai-node-red
    networks: ['lai']
    restart: unless-stopped
    ports:
      - "1880:1880"
    volumes:
      - ./node-red/data:/data:rw
    environment:
      - TZ=UTC

  # Perplexica Backend
  perplexica-backend:
    build:
      context: ./perplexica
      dockerfile: backend.dockerfile
    container_name: lai-perplexica-backend
    networks: ['lai']
    environment:
      - SEARXNG_API_URL=http://lai-searxng:8080
    depends_on:
      - searxng
      - ollama
    ports:
      - 3001:3001
    volumes:
      - ./perplexica/data:/home/perplexica/data:rw
      - ./perplexica/uploads:/home/perplexica/uploads:rw
      - ./config.toml:/home/perplexica/config.toml:ro
    extra_hosts:
      - 'host.docker.internal:host-gateway'
    restart: unless-stopped
    profiles: ["cpu"]

  # Perplexica Frontend
  perplexica-frontend:
    build:
      context: ./perplexica
      dockerfile: app.dockerfile
      args:
        - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
        - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
    container_name: lai-perplexica-frontend
    networks: ['lai']
    depends_on:
      - perplexica-backend
    ports:
      - 3000:3000
    restart: unless-stopped
    profiles: ["cpu"]
