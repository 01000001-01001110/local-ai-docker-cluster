# Main docker-compose.yml file for the AI development environment
# This file orchestrates several services that work together to provide
# a comprehensive AI development platform

# Define the main network our services will use to communicate
networks:
  lai:
    name: lai_local_ai_server
    driver: bridge

# Base configuration for n8n services that will be reused
x-n8n: &service-n8n
  image: n8nio/n8n:latest
  networks: ['lai']
  environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=postgres
    - DB_POSTGRESDB_USER=${POSTGRES_USER}
    - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
    - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY
    - N8N_USER_MANAGEMENT_JWT_SECRET
    - N8N_USER_MANAGEMENT_DISABLED=true
    - NODE_ENV=production
    - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
  links:
    - postgres

# Base configuration for Ollama services
x-ollama: &service-ollama
  image: ollama/ollama:latest
  networks: ['lai']
  restart: unless-stopped
  ports:
    - 11434:11434
  volumes:
    - ./ollama/models:/root/.ollama:rw
  deploy:
    resources:
      limits:
        memory: 8G
      reservations:
        memory: 2G
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 30s

# Main services section defining all our containers
services:
  # MongoDB with Vector Search capabilities
  mongodb:
    image: mongo:7.0
    container_name: lai-mongodb
    networks: ['lai']
    restart: unless-stopped
    ports:
      - 27017:27017
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGODB_USER:-root}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD:-password}
      - MONGO_INITDB_DATABASE=${MONGODB_DATABASE:-langchain_db}
    volumes:
      - ./mongodb/data:/data/db:rw
      - ./mongodb/init:/docker-entrypoint-initdb.d:ro
    command: mongod --bind_ip_all
    deploy:
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Flowise - Low code AI flow builder
  flowise:
    image: flowiseai/flowise
    networks: ['lai']
    restart: unless-stopped
    container_name: lai-flowise
    environment:
        - PORT=3001
        - FLOWISE_DATA_PATH=/root/.flowise
        - FLOWISE_STORAGE_PATH=/root/.flowise-storage
        - MONGODB_URI=mongodb://${MONGODB_USER:-root}:${MONGODB_PASSWORD:-password}@lai-mongodb:27017/${MONGODB_DATABASE:-langchain_db}?authSource=admin
    ports:
        - 3003:3001
    extra_hosts:
      - "host.docker.internal:host-gateway"        
    volumes:
        - ./flowise/data:/root/.flowise:rw
        - ./flowise/storage:/root/.flowise-storage:rw
    deploy:
      resources:
        limits:
          memory: 2G
    entrypoint: /bin/sh -c "sleep 3; flowise start"
    depends_on:
      mongodb:
        condition: service_healthy

  # Open WebUI - Interface for interacting with AI models
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    networks: ['lai']
    restart: unless-stopped
    container_name: lai-open-webui
    ports:
      - "3002:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./open-webui/data:/app/backend/data:rw
    deploy:
      resources:
        limits:
          memory: 1G
    profiles: ["gpu-nvidia", "cpu"]

  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: lai-postgres
    networks: ['lai']
    restart: unless-stopped
    ports:
      - 5432:5432
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
    volumes:
      - ./postgres/data/pgdata:/var/lib/postgresql/data:rw
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U ${POSTGRES_USER:-root} -d ${POSTGRES_DB:-n8n}']
      interval: 5s
      timeout: 5s
      retries: 10

  # Main n8n Service
  n8n:
    <<: *service-n8n
    container_name: lai-n8n
    restart: unless-stopped
    ports:
      - 5678:5678
    volumes:
      - ./n8n/data:/home/node/.n8n:rw
      - ./n8n/backup:/backup:ro
      - ./shared:/data/shared:rw
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_USER=${POSTGRES_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_PERSONALIZATION_ENABLED=false
      - N8N_ENCRYPTION_KEY
      - N8N_USER_MANAGEMENT_JWT_SECRET
      - N8N_USER_MANAGEMENT_DISABLED=true
      - NODE_ENV=production
      - MONGODB_URI=mongodb://${MONGODB_USER:-root}:${MONGODB_PASSWORD:-password}@lai-mongodb:27017/${MONGODB_DATABASE:-langchain_db}?authSource=admin
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
    networks:
      - lai
    deploy:
      resources:
        limits:
          memory: 2G
    depends_on:
      postgres:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:5678/healthz"]
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 30s
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        chmod 600 /home/node/.n8n/config || true
        exec node /usr/local/lib/node_modules/n8n/bin/n8n

  # n8n Workflow Importer
  n8n-import:
    <<: *service-n8n
    container_name: lai-n8n-import
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        echo "Waiting for n8n to be ready..."
        while ! wget --spider -q http://n8n:5678/healthz; do
          sleep 5
        done
        echo "n8n is ready, starting import..."
        sleep 5
        n8n import:credentials --separate --input=/backup/credentials || true
        n8n import:workflow --separate --input=/backup/workflows || true
        echo "Import complete"
    volumes:
      - ./n8n/backup:/backup:ro
    depends_on:
      n8n:
        condition: service_healthy

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant
    container_name: lai-qdrant
    networks: ['lai']
    restart: unless-stopped
    ports:
      - 6333:6333
    volumes:
      - ./qdrant/storage:/qdrant/storage:rw
    deploy:
      resources:
        limits:
          memory: 2G

  # Ollama CPU Version
  ollama:
    <<: *service-ollama
    container_name: lai-ollama
    profiles: ["cpu"]

  # Ollama GPU Version
  ollama-gpu:
    <<: *service-ollama
    container_name: lai-ollama-gpu
    profiles: ["gpu-nvidia"]
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Initialize Ollama Models (CPU)
  ollama-init:
    image: curlimages/curl:latest
    networks: ['lai']
    container_name: lai-ollama-init
    profiles: ["cpu"]
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        echo "Waiting for Ollama to be ready..."
        while ! curl -s http://lai-ollama:11434/api/version > /dev/null; do
          sleep 5
        done
        echo "Ollama is ready, pulling models..."
        curl -X POST http://lai-ollama:11434/api/pull -d '{"name":"tinyllama"}'
        curl -X POST http://lai-ollama:11434/api/pull -d '{"name":"nomic-embed-text"}'
    depends_on:
      ollama:
        condition: service_healthy

  # Initialize Ollama Models (GPU)
  ollama-init-gpu:
    image: curlimages/curl:latest
    networks: ['lai']
    container_name: lai-ollama-init-gpu
    profiles: ["gpu-nvidia"]
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        echo "Waiting for Ollama to be ready..."
        max_attempts=30
        attempt=1
        while ! curl -s http://lai-ollama-gpu:11434/api/version > /dev/null; do
          echo "Attempt $attempt of $max_attempts: Ollama not ready yet..."
          if [ $attempt -ge $max_attempts ]; then
            echo "Ollama failed to start after $max_attempts attempts"
            exit 1
          fi
          attempt=$((attempt + 1))
          sleep 10
        done
        echo "Ollama is ready, pulling models..."
        
        # Pull llama2 model
        echo "Pulling llama2..."
        curl -X POST http://lai-ollama-gpu:11434/api/pull -d '{"name":"llama2"}' || {
          echo "Failed to pull llama2"
          exit 1
        }
        
        # Pull nomic-embed-text model
        echo "Pulling nomic-embed-text..."
        curl -X POST http://lai-ollama-gpu:11434/api/pull -d '{"name":"nomic-embed-text"}' || {
          echo "Failed to pull nomic-embed-text"
          exit 1
        }
        
        echo "All models pulled successfully"
    depends_on:
      ollama-gpu:
        condition: service_healthy

  # Nginx Proxy Manager
  nginx-proxy-manager:
    image: 'jc21/nginx-proxy-manager:latest'
    container_name: lai-nginx
    networks: ['lai']
    restart: unless-stopped
    ports:
      - '80:80'
      - '81:81'
      - '443:443'
    volumes:
      - ./nginx/data:/data:rw
      - ./nginx/letsencrypt:/etc/letsencrypt:rw
    deploy:
      resources:
        limits:
          memory: 512M

  # Node-RED
  node-red:
    image: nodered/node-red:latest
    container_name: lai-node-red
    networks: ['lai']
    restart: unless-stopped
    ports:
      - "1880:1880"
    volumes:
      - ./node-red/data:/data:rw
    environment:
      - TZ=UTC
    deploy:
      resources:
        limits:
          memory: 1G
